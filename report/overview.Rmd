# Performance Results

The last complete run of this benchmark setup yielded the results presented
below. This report was generated on `r Sys.time()`.

```{r load-scripts, echo=FALSE, include=FALSE}
# load libraries, the data, and prepare it
if (Sys.getenv("RSTUDIO") == "1") { setwd("/Users/smarr/Projects/are-we-fast-yet/report") }
source("scripts/libraries.R", chdir=TRUE)
opts_chunk$set(dev = 'png',
               dev.args=list(pointsize=10),
               echo = FALSE,
               fig.keep='all',
               fig.path="figures/",
               external=FALSE,
               tidy=FALSE)
#    cache=TRUE,

vm_names <- c(
  "Crystal"               = "Crystal",
  "GraalBasic"            = "Graal Core",
  "GraalC2"               = "HotSpot C2, jvmi",
  "GraalEnterprise"       = "Graal VM",
  "GraalJS"               = "Graal.js",
  
  "JRubyC2"               = "JRuby (C2, jvmci)",
  "JRubyGraal"            = "JRuby (Graal Core)",
  "JRubyJ8"               = "JRuby (C2, Java)",
  
  "Topaz"                 = "Topaz",
  
  "TruffleRuby-jit"       = "TruffleRuby 21.1",
  "JRubyTruffleEnterprise" = "TruffleRuby (GraalVM)",
  
  "Java8U66"              = "HotSpot 1.8 292",
  "JavaInt"               = "HotSpot 1.8 292 Int",
  
  "Lua53"                 = "Lua 5.3.3",
  "LuaJIT2"               = "LuaJIT 2",
  "MRI23"                 = "MRI 2.5.1",
  "Node"                  = "Node 14.17",
  "Node-interp"           = "Node 14.17 Int",
  "Pharo"                 = "Pharo",
  "RBX314"                = "Rubinius",
  "SOMns"                 = "SOMns core",
  "SOMns-Enterprise"      = "SOMns GraalVM",
  "SOMnsInt"              = "SOMns int",
  "TruffleSOM"            = "TruffleSOM core",
  "TruffleSOM-Enterprise" = "TruffleSOM GraalVM",
  "TruffleSOM-TOM"        = "TruffleSOM core, Truffle OM",
  "TruffleSOM-TOM-Enterprise" = "TruffleSOM GraalVM, Truffle OM",
  
  "CPython-interp"        = "CPython 3.10",
  "Cinder-jit"            = "Cinder 3.8",
  "Pyston-jit"            = "Pyston 3.8",
  "GraalPython-jit"       = "GraalPython 21.1",
  "PyPy-jit"              = "PyPy 3.7 7.3.4"
  )

vms_all  <- names(vm_names)
vms_slow <- c("JRubyC2", "JRubyGraal", "JRubyJ8",
              "JavaInt",
              "Lua53", "LuaJIT2",
              "MRI23", "RBX314",
              "CPython-interp",
              "Node-interp",
              "Pyston-jit", "Cinder-jit",
              "Pharo", "SOMnsInt", "Topaz")
vms_fast <- c("Crystal",
              "GraalBasic", "GraalC2", "GraalEnterprise",
              "GraalJS",
              "TruffleRuby-jit", "JRubyTruffleEnterprise",
              "Java8U66",
              "PyPy-jit",
              "GraalPython-jit",
              "Node",
              "SOMns", "SOMns-Enterprise",
              "TruffleSOM", "TruffleSOM-TOM", "TruffleSOM-Enterprise", "TruffleSOM-TOM-Enterprise")
vms_truffle <- c("GraalJS",
                 "TruffleRuby-jit", "JRubyTruffleEnterprise",
                 "GraalPython-jit",
                 "TruffleSOM", "TruffleSOM-Enterprise", "TruffleSOM-TOM", "TruffleSOM-TOM-Enterprise",
                 "SOMns", "SOMns-Enterprise")

assert_that(all(sort(c(vms_slow, vms_fast)) == sort(vms_all))) ## sanity check

# vm_colors <- brewer.pal(length(vms_all), "Paired")  # to replace scale_fill_brewer(type = "qual", palette = "Paired")
vm_colors <- rainbow(length(vms_all))
vm_colors_light <- rainbow(length(vms_all), s = 0.8, v = 0.8)

names(vm_colors) <- vm_names
names(vm_colors_light) <- vm_names

data <- load_all_data("data", "AWFY-")

warmup_slow <- 100
warmup_fast <- 500

data_fast_vms <- data %>%
  filter(iteration >= warmup_fast & iteration <= 1000 & exe %in% vms_fast) %>%
  droplevels()

data_very_slow_vms <- data %>%
  filter(exe %in% vms_slow & exe != "JRubyJ8") %>%
  droplevels()
  
data_slow_vms <- data %>%
  filter(iteration >= warmup_slow & (exe == "JRubyJ8" | exe == "Pharo")) %>%
  droplevels()
data <- rbind(data_fast_vms, data_slow_vms, data_very_slow_vms) %>%
  select(-c(runid, trialid, commitid, suite, cmdline))


base <- data %>%
  filter(exe == "Java8U66") %>%
  group_by(bench,
           varvalue, cores, inputsize, extraargs) %>%
  summarise(base_mean = mean(value),
            base_median = median(value),
            .groups = "drop")


norm <- data %>%
  left_join(base, by = c("bench",
                         "varvalue", "cores", "inputsize", "extraargs")) %>%
  group_by(exe, bench,
           varvalue, cores, inputsize, extraargs) %>%
  transform(ratio_mean = value / base_mean,
            ratio_median = value / base_median)
  
  
  
stats <- norm %>%
  group_by(expid,
           exe, bench,
           varvalue, cores, inputsize, extraargs) %>%
  summarise(
    unit = unit[1],
    median = median(value),
    min = min(value),
    max = max(value),
    samples = length(value),
    ratio = median / base_median[1],
    change_m = ratio - 1,
    .groups = "drop")
  


stats_vm <- stats %>%
  group_by(exe, expid) %>%
  summarise(
    unit = unit[1],
    median_ratio = median(ratio),
    min_ratio = min(ratio),
    max_ratio = max(ratio),
    .groups = "drop")
  
stats_vm_latest <- stats_vm %>%
  transform(expid = as.integer(as.character(expid))) %>%
  group_by(exe) %>%
  filter(expid == max(expid))

norm_latest <- norm %>%
  transform(expid = as.integer(as.character(expid))) %>%
  group_by(exe) %>%
  filter(expid == max(expid)) %>%
  left_join(stats_vm_latest %>%
              transmute(vm_ratio = median_ratio), by = c("exe")) %>%
  droplevels()

stats_latest <- stats %>%
  transform(expid = as.integer(as.character(expid))) %>%
  group_by(exe) %>%
  filter(expid == max(expid)) %>%
  left_join(stats_vm_latest %>%
              transmute(vm_ratio = median_ratio), by = c("exe")) %>%
  droplevels()


plot_benchmarks_speedup_for_vms <- function(
  vm_norm, vms, label = "Runtime Factor, normalized to Java\n(lower is better)") {
  # vm_norm <- norm_latest
  # vms <- vms_fast
  
  vm_norm <- vm_norm %>%
    filter(exe %in% vms) %>%
    droplevels()
  suppressMessages(vm_norm$exe <- revalue(vm_norm$exe, vm_names))
  vm_norm$exe <- reorder(vm_norm$exe, X=vm_norm$vm_ratio)

  breaks <- levels(droplevels(vm_norm)$exe)
  col_values <- sapply(breaks, function(x) vm_colors[[x]])
  
  for (b in levels(vm_norm$bench)) {
    # b <- "Bounce"
    data_b <- droplevels(filter(vm_norm, bench == b))

    p <- ggplot(data_b, aes(y = exe, x = ratio_median, fill = exe)) +
      geom_vline(aes(xintercept=1), colour="#333333", linetype="solid") +
      geom_boxplot(aes(color = exe),
                   outlier.size = 0.9,
                   outlier.alpha = 0.6,
                   lwd=0.2) +
      geom_jitter(aes(color = exe, y = exe), size=0.3, alpha=0.3) +
      scale_x_log10() +
      scale_y_discrete(limits = rev) +
      scale_color_manual(values = vm_colors) +
      scale_fill_manual(values = vm_colors_light) +
      theme_simple() + # scale_fill_manual(values=col) +
      theme(legend.position="none",
            axis.title.x = element_text(
              size = 12,
              margin = margin(t = 0.1, unit = "cm"))) +
      ggtitle(b) +
      xlab(label) 
      
    tryCatch({print(p)})
  }
}

overview_box_plot <- function(stats, vms, prepare_data = NULL, pre_plot = NULL, new_colors = FALSE) {
  # stats <- stats_latest
  # vms <- c("Node", "Pharo", "JavaInt", 
  #          "Lua53", "LuaJIT2", "SOMns-Enterprise")

  vm_stats <- stats %>%
    filter(exe %in% vms)

  suppressMessages(vm_stats$exe <- revalue(vm_stats$exe, vm_names))
  vm_stats$exe <- reorder(vm_stats$exe, X=-vm_stats$vm_ratio)
  if (!is.null(prepare_data)) {
   vm_stats <- prepare_data(vm_stats)
  }
  vm_stats <- droplevels(vm_stats)

  breaks <- levels(vm_stats$exe)
  cat(breaks)
  if (new_colors) {
    col_values <- brewer.pal(length(breaks), "Paired")
  } else {
    col_values <- sapply(breaks, function(x) vm_colors[[x]])
  }

  plot <- ggplot(vm_stats, aes(x=exe, y=ratio, fill = exe))
  if (!is.null(pre_plot)) {
    plot <- pre_plot(plot)
  }
  plot <- plot +
    geom_boxplot(outlier.size = 0.5) + #fill=get_color(5, 7)
    theme_bw() + theme_simple(font_size = 8) +
    theme(axis.text.x = element_text(angle= 90, vjust=0.5, hjust=1), legend.position="none") +
    #scale_y_log10(breaks=c(1,2,3,10,20,30,50,100,200,300,500,1000)) + #limit=c(0,30), breaks=seq(0,100,5), expand = c(0,0)
    ggtitle("Runtime Factor, normalized to Java\n(lower is better)") + coord_flip() + xlab("") +
    scale_fill_manual(values = col_values)
  plot
}
```

All results are normalized to Java 1.8.0_292. Furthermore, we report afer 
some warmup. This means, the reported measurements are taken after `r warmup_slow` or `r warmup_fast` warmup iterations.

## Overview

##### Fast Language Implementations

The following set of language implementations reaches the performance of Java on
our set of benchmarks within a factor of 2 to 3 on average. To allow for a more
detailed assessment of these *fast* language implementations, we exclude slower
ones from the following plot.

```{r fast-langs-overview, fig.width=15, fig.height=6}
# overview_box_plot(stats_latest,
#                   c("Node", "Pharo", "JavaInt", "Java8U66", "MRI23", "Lua53", "LuaJIT2", "SOMns-Enterprise"), new_colors = TRUE)
p <- overview_box_plot(stats_latest, vms_fast, pre_plot = function (p) {
  p + geom_hline(aes(yintercept=1), colour="#cccccc", linetype="dashed") +
      geom_hline(aes(yintercept=2), colour="#cccccc", linetype="dashed") +
      geom_hline(aes(yintercept=3), colour="#cccccc", linetype="dashed") })
p + scale_y_continuous(limit=c(0,13), breaks = c(1, 2, 3, 4, 6, 8, 10, 12))
```

##### All Language Implementations

Other language implementations are not necessarily reaching performance similar
to Java on our benchmarks. The following plot include all of the
implementations.

```{r all-langs-overview, fig.width=8, fig.height=10}
p <- overview_box_plot(stats_latest, vms_all)
p + scale_y_continuous(breaks = c(0, 5, seq(from=10, by=10, to=100)))
```

###### Ruby Performance

```{r ruby-overview, fig.width=8, fig.height=6}
p <- overview_box_plot(stats_latest, c("Crystal", "Java8U66", "RBX314", "Topaz", "MRU23", "JRubyC2", "JRubyTruffleEnterprise", "Pharo"))
p + scale_y_continuous(breaks = c(0, 5, seq(from=10, by=10, to=100)))
```

##### Performance Overview Data
<a id="data-table"></a>

The following table contains the numerical representation of the results
depicted above.

```{r truffle-lang-table, results='asis', echo=FALSE}
vm_stats <- stats_vm_latest
suppressMessages(vm_stats$exe <- revalue(vm_stats$exe, vm_names))
vm_stats$exe <- reorder(vm_stats$exe, X=vm_stats$median_ratio)


t <- tabular(Justify("l")*Heading()*exe ~
             Heading('Runtime Factor over Java')*Justify("r")*Format(sprintf("%.2f"))*((median_ratio + min_ratio + max_ratio)*Heading()*identity), data=vm_stats)
table_options(justification="c ")
html.tabular(t)
```

## Details for all Benchmarks
<a id="all-benchmarks"></a>

The following plots show results for each of the benchmarks.

##### Fast Language Implementations

```{r fast-langs-benchmarks, fig.width=6, fig.height=3}
plot_benchmarks_speedup_for_vms(norm_latest, vms_fast)
```

##### Slow Language Implementations

```{r slow-langs-benchmarks, fig.width=6, fig.height=4}
plot_benchmarks_speedup_for_vms(norm_latest, vms_slow)
```

##### Benchmark Results
<a id="benchmark-table"></a>

The following table contains the numerical results for all benchmarks.

```{r benchmark-table, results='asis', echo=FALSE}
t_stats <- stats_latest
suppressMessages(t_stats$exe <- revalue(t_stats$exe, vm_names))
t_stats$exe <- reorder(t_stats$exe, X=t_stats$vm_ratio)

show_plain <- mean ## this is silly, but works better than the identity for missing values

t <- tabular(Justify("l")*Heading()*bench*exe ~
             Heading('Runtime Factor over Java')*Justify("r")*Format(sprintf("%.2f"))*((
                 Heading("mean")*ratio
               # + Heading("sd")*RR.sd
               # + Heading("median")*RR.median
               )*Heading()*show_plain), data=t_stats)
html.tabular(t)
```
